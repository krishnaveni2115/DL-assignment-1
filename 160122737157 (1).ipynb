{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0EL_ULwnpsN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Load MNIST dataset\n",
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values\n",
        "    x_train = x_train.reshape(-1, 28 * 28)\n",
        "    x_test = x_test.reshape(-1, 28 * 28)\n",
        "    y_train = keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
        "\n",
        "# Plot sample images\n",
        "def plot_samples(x_train):\n",
        "    fig, axes = plt.subplots(1, 10, figsize=(10, 2))\n",
        "    for i in range(10):\n",
        "        axes[i].imshow(x_train[i].reshape(28, 28), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_model(hidden_layers=3, hidden_units=64, activation='relu', optimizer='adam', weight_init='random'):\n",
        "    initializer = keras.initializers.RandomNormal() if weight_init == 'random' else keras.initializers.GlorotNormal()\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Input(shape=(28 * 28,)))\n",
        "\n",
        "    for _ in range(hidden_layers):\n",
        "        model.add(layers.Dense(hidden_units, activation=activation, kernel_initializer=initializer))\n",
        "\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "    optimizers_dict = {\n",
        "        'sgd': optimizers.SGD(),\n",
        "        'momentum': optimizers.SGD(momentum=0.9),\n",
        "        'nesterov': optimizers.SGD(momentum=0.9, nesterov=True),\n",
        "        'rmsprop': optimizers.RMSprop(),\n",
        "        'adam': optimizers.Adam(),\n",
        "        'nadam': optimizers.Nadam()\n",
        "    }\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers_dict[optimizer], metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_model(x_train, y_train, x_val, y_val, model, batch_size=32, epochs=10):\n",
        "    history = model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "    return history\n",
        "\n",
        "# Evaluate model and plot confusion matrix\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "    acc = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    print(f'Test Accuracy: {acc * 100:.2f}%')\n",
        "\n",
        "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compare_loss_functions():\n",
        "    model_ce = create_model()\n",
        "    model_mse = create_model()\n",
        "    model_mse.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    history_ce = model_ce.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=32, verbose=1)\n",
        "    history_mse = model_mse.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=5, batch_size=32, verbose=1)\n",
        "\n",
        "    plt.plot(history_ce.history['val_loss'], label='Cross Entropy')\n",
        "    plt.plot(history_mse.history['val_loss'], label='Mean Squared Error')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "x_train, y_train, x_val, y_val, x_test, y_test = load_data()\n",
        "plot_samples(x_train)\n",
        "model = create_model(hidden_layers=3, hidden_units=64, activation='relu', optimizer='adam', weight_init='xavier')\n",
        "history = train_model(x_train, y_train, x_val, y_val, model, batch_size=32, epochs=10)\n",
        "evaluate_model(model, x_test, y_test)\n",
        "compare_loss_functions()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVMHUCrNnqud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project trains a feedforward neural network to classify handwritten digits from the MNIST dataset. The dataset consists of 70,000 grayscale images (28x28 pixels), divided into 60,000 training samples and 10,000 test samples.\n",
        "\n",
        "\n",
        "Input Layer: 784 neurons (flattened 28x28 images)\n",
        "Hidden Layers: 3 fully connected layers with 64 neurons each\n",
        "Activation Function: ReLU (Rectified Linear Unit)\n",
        "Output Layer: 10 neurons with softmax activation (multi-class classification)\n",
        "Weight Initialization: Xavier (Glorot Normal)\n",
        "Optimizer: Adam\n",
        "\n",
        "\n",
        "\n",
        "Results and Observations\n",
        "1. Training Performance\n",
        "The model was trained for 10 epochs with a batch size of 32.\n",
        "The training loss decreased steadily, indicating that the model was learning well.\n",
        "The validation accuracy remained relatively stable, suggesting no major overfitting.\n",
        "2. Test Performance\n",
        "The final test accuracy was approximately 98%, which is competitive for a basic neural network on MNIST.\n",
        "The confusion matrix showed that most errors occurred between visually similar digits (e.g., 3 vs. 8, 4 vs. 9).\n",
        "3. Comparison of Loss Functions\n",
        "Cross-Entropy Loss resulted in faster convergence and better overall accuracy.\n",
        "Mean Squared Error (MSE) performed worse, as it is not ideal for classification problems.\n",
        "The validation loss was consistently lower for cross-entropy compared to MSE.\n",
        "Conclusion\n",
        "The model performed well, achieving high accuracy on test data.\n",
        "Using cross-entropy loss is preferable for classification over MSE.\n",
        "Additional improvements could include convolutional layers (CNNs), dropout regularization, or hyperparameter tuning for better generalization.\n"
      ],
      "metadata": {
        "id": "_gVaymwrp5Bn"
      }
    }
  ]
}